{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üöÄ kangtani.ai - Agricultural Chatbot Assistant\n",
                "\n",
                "This notebook runs the complete kangtani.ai project (backend + frontend) in Google Colab with ngrok port forwarding.\n",
                "\n",
                "## What this does:\n",
                "- Clones the GitHub repository\n",
                "- Installs all dependencies\n",
                "- Runs Ollama with Gemma model\n",
                "- Starts FastAPI backend\n",
                "- Starts Next.js frontend\n",
                "- Sets up ngrok for public access\n",
                "\n",
                "## Prerequisites:\n",
                "- GitHub repository URL (you'll need to provide this)\n",
                "- ngrok authtoken (optional, for custom domains)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup"
            },
            "source": [
                "## üìã Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_dependencies"
            },
            "outputs": [],
            "source": [
                "# Install system dependencies\n",
                "!apt-get update\n",
                "!apt-get install -y curl wget git build-essential\n",
                "\n",
                "# Install Node.js and npm\n",
                "!curl -fsSL https://deb.nodesource.com/setup_18.x | bash -\n",
                "!apt-get install -y nodejs\n",
                "\n",
                "# Install pnpm\n",
                "!npm install -g pnpm\n",
                "\n",
                "# Install Ollama\n",
                "!curl -fsSL https://ollama.ai/install.sh | sh\n",
                "\n",
                "# Install ngrok\n",
                "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
                "!tar xvzf ngrok-v3-stable-linux-amd64.tgz\n",
                "!mv ngrok /usr/local/bin/\n",
                "\n",
                "# Install Python dependencies for backend\n",
                "!pip install fastapi uvicorn[standard] httpx python-multipart\n",
                "!pip install openai-whisper PyPDF2 pdfplumber python-docx\n",
                "\n",
                "print(\"‚úÖ All dependencies installed successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone_repo"
            },
            "outputs": [],
            "source": [
                "# Clone the GitHub repository\n",
                "# Replace with your actual GitHub repository URL\n",
                "GITHUB_REPO = \"https://github.com/yourusername/kangtani.ai.git\"\n",
                "\n",
                "# You can also use a direct download if the repo is public\n",
                "# GITHUB_REPO = \"https://github.com/yourusername/kangtani.ai/archive/refs/heads/main.zip\"\n",
                "\n",
                "print(f\"üì• Cloning repository: {GITHUB_REPO}\")\n",
                "\n",
                "if GITHUB_REPO.endswith('.git'):\n",
                "    !git clone {GITHUB_REPO}\n",
                "    !cd kangtani.ai && ls -la\n",
                "else:\n",
                "    !wget {GITHUB_REPO} -O kangtani.zip\n",
                "    !unzip kangtani.zip\n",
                "    !ls -la\n",
                "\n",
                "print(\"‚úÖ Repository cloned successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup_ngrok"
            },
            "outputs": [],
            "source": [
                "# Setup ngrok (optional - for custom domains)\n",
                "# If you have an ngrok authtoken, uncomment and add it below\n",
                "# NGROK_AUTHTOKEN = \"your_ngrok_authtoken_here\"\n",
                "# !ngrok config add-authtoken {NGROK_AUTHTOKEN}\n",
                "\n",
                "print(\"üîß ngrok configured (using free tier)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ollama_setup"
            },
            "source": [
                "## ü§ñ Ollama Setup & Model Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "start_ollama"
            },
            "outputs": [],
            "source": [
                "# Start Ollama service in background\n",
                "import subprocess\n",
                "import time\n",
                "import requests\n",
                "\n",
                "print(\"üöÄ Starting Ollama service...\")\n",
                "\n",
                "# Start Ollama in background\n",
                "ollama_process = subprocess.Popen([\"ollama\", \"serve\"], \n",
                "                                  stdout=subprocess.PIPE, \n",
                "                                  stderr=subprocess.PIPE)\n",
                "\n",
                "# Wait for Ollama to start\n",
                "time.sleep(5)\n",
                "\n",
                "# Test Ollama connection\n",
                "try:\n",
                "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
                "    if response.status_code == 200:\n",
                "        print(\"‚úÖ Ollama service is running!\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è  Ollama service might not be ready yet...\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è  Ollama service starting...\")\n",
                "\n",
                "print(\"üì• Downloading Gemma model (this may take a while)...\")\n",
                "!ollama pull gemma\n",
                "\n",
                "print(\"‚úÖ Gemma model downloaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "backend_setup"
            },
            "source": [
                "## üîß Backend Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_backend"
            },
            "outputs": [],
            "source": [
                "# Navigate to backend directory and install dependencies\n",
                "!cd kangtani.ai/backend && pip install -r requirements.txt\n",
                "\n",
                "print(\"‚úÖ Backend dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "start_backend"
            },
            "outputs": [],
            "source": [
                "# Start FastAPI backend in background\n",
                "import subprocess\n",
                "import time\n",
                "import requests\n",
                "\n",
                "print(\"üöÄ Starting FastAPI backend...\")\n",
                "\n",
                "# Start backend in background\n",
                "backend_process = subprocess.Popen([\n",
                "    \"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"\n",
                "], cwd=\"kangtani.ai/backend\", \n",
                "   stdout=subprocess.PIPE, \n",
                "   stderr=subprocess.PIPE)\n",
                "\n",
                "# Wait for backend to start\n",
                "time.sleep(10)\n",
                "\n",
                "# Test backend connection\n",
                "try:\n",
                "    response = requests.get(\"http://localhost:8000/health\")\n",
                "    if response.status_code == 200:\n",
                "        print(\"‚úÖ Backend is running!\")\n",
                "        print(f\"üìä Health check: {response.json()}\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è  Backend might not be ready yet...\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è  Backend starting... Error: {e}\")\n",
                "\n",
                "print(\"üåê Backend URL: http://localhost:8000\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "frontend_setup"
            },
            "source": [
                "## üé® Frontend Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_frontend"
            },
            "outputs": [],
            "source": [
                "# Navigate to frontend directory and install dependencies\n",
                "!cd kangtani.ai/frontend && pnpm install\n",
                "\n",
                "print(\"‚úÖ Frontend dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "start_frontend"
            },
            "outputs": [],
            "source": [
                "# Start Next.js frontend in background\n",
                "import subprocess\n",
                "import time\n",
                "import requests\n",
                "\n",
                "print(\"üöÄ Starting Next.js frontend...\")\n",
                "\n",
                "# Start frontend in background\n",
                "frontend_process = subprocess.Popen([\n",
                "    \"pnpm\", \"dev\", \"--hostname\", \"0.0.0.0\", \"--port\", \"3000\"\n",
                "], cwd=\"kangtani.ai/frontend\", \n",
                "   stdout=subprocess.PIPE, \n",
                "   stderr=subprocess.PIPE)\n",
                "\n",
                "# Wait for frontend to start\n",
                "time.sleep(15)\n",
                "\n",
                "# Test frontend connection\n",
                "try:\n",
                "    response = requests.get(\"http://localhost:3000\")\n",
                "    if response.status_code == 200:\n",
                "        print(\"‚úÖ Frontend is running!\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è  Frontend might not be ready yet...\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è  Frontend starting... Error: {e}\")\n",
                "\n",
                "print(\"üåê Frontend URL: http://localhost:3000\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ngrok_setup"
            },
            "source": [
                "## üåç ngrok Port Forwarding Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "start_ngrok"
            },
            "outputs": [],
            "source": [
                "# Start ngrok for public access\n",
                "import subprocess\n",
                "import time\n",
                "import requests\n",
                "import json\n",
                "\n",
                "print(\"üåç Setting up ngrok port forwarding...\")\n",
                "\n",
                "# Start ngrok for frontend (port 3000)\n",
                "ngrok_frontend = subprocess.Popen([\n",
                "    \"ngrok\", \"http\", \"3000\"\n",
                "], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "\n",
                "# Wait for ngrok to start\n",
                "time.sleep(5)\n",
                "\n",
                "# Get ngrok public URL\n",
                "try:\n",
                "    response = requests.get(\"http://localhost:4040/api/tunnels\")\n",
                "    if response.status_code == 200:\n",
                "        tunnels = response.json()\n",
                "        if tunnels['tunnels']:\n",
                "            public_url = tunnels['tunnels'][0]['public_url']\n",
                "            print(f\"‚úÖ ngrok is running!\")\n",
                "            print(f\"üåê Public URL: {public_url}\")\n",
                "            print(f\"üîó ngrok dashboard: http://localhost:4040\")\n",
                "        else:\n",
                "            print(\"‚ö†Ô∏è  ngrok tunnel not ready yet...\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è  ngrok API not accessible...\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è  ngrok starting... Error: {e}\")\n",
                "\n",
                "print(\"\\nüìù Note: The frontend will be accessible via the ngrok URL above.\")\n",
                "print(\"üìù The backend API will be available at: {public_url.replace('https://', 'https://').replace('http://', 'https://').replace(':3000', ':8000')}\")\n",
                "print(\"üìù You may need to update the frontend API URL to use the ngrok backend URL.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "status_check"
            },
            "source": [
                "## üìä Status Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "check_status"
            },
            "outputs": [],
            "source": [
                "# Check status of all services\n",
                "import requests\n",
                "import subprocess\n",
                "\n",
                "print(\"üîç Checking service status...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Check Ollama\n",
                "try:\n",
                "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
                "    if response.status_code == 200:\n",
                "        print(\"‚úÖ Ollama: Running\")\n",
                "        models = response.json().get('models', [])\n",
                "        if models:\n",
                "            print(f\"   üì¶ Models: {', '.join([m['name'] for m in models])}\")\n",
                "    else:\n",
                "        print(\"‚ùå Ollama: Not responding\")\n",
                "except:\n",
                "    print(\"‚ùå Ollama: Not accessible\")\n",
                "\n",
                "# Check Backend\n",
                "try:\n",
                "    response = requests.get(\"http://localhost:8000/health\")\n",
                "    if response.status_code == 200:\n",
                "        print(\"‚úÖ Backend: Running\")\n",
                "        health = response.json()\n",
                "        print(f\"   üîó Health: {health.get('status', 'unknown')}\")\n",
                "    else:\n",
                "        print(\"‚ùå Backend: Not responding\")\n",
                "except:\n",
                "    print(\"‚ùå Backend: Not accessible\")\n",
                "\n",
                "# Check Frontend\n",
                "try:\n",
                "    response = requests.get(\"http://localhost:3000\")\n",
                "    if response.status_code == 200:\n",
                "        print(\"‚úÖ Frontend: Running\")\n",
                "    else:\n",
                "        print(\"‚ùå Frontend: Not responding\")\n",
                "except:\n",
                "    print(\"‚ùå Frontend: Not accessible\")\n",
                "\n",
                "# Check ngrok\n",
                "try:\n",
                "    response = requests.get(\"http://localhost:4040/api/tunnels\")\n",
                "    if response.status_code == 200:\n",
                "        tunnels = response.json()\n",
                "        if tunnels['tunnels']:\n",
                "            print(\"‚úÖ ngrok: Running\")\n",
                "            print(f\"   üåê Public URL: {tunnels['tunnels'][0]['public_url']}\")\n",
                "        else:\n",
                "            print(\"‚ùå ngrok: No tunnels\")\n",
                "    else:\n",
                "        print(\"‚ùå ngrok: Not responding\")\n",
                "except:\n",
                "    print(\"‚ùå ngrok: Not accessible\")\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"üéâ Setup complete! Your kangtani.ai is now running.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "usage"
            },
            "source": [
                "## üéØ Usage Instructions\n",
                "\n",
                "1. **Access the application**: Use the ngrok public URL provided above\n",
                "2. **Start chatting**: The chatbot will respond using the Gemma model via Ollama\n",
                "3. **Monitor logs**: Check the ngrok dashboard at `http://localhost:4040`\n",
                "4. **Stop services**: Run the cell below to stop all services\n",
                "\n",
                "### API Endpoints:\n",
                "- Frontend: `{ngrok_url}`\n",
                "- Backend API: `{ngrok_url.replace(':3000', ':8000')}`\n",
                "- Health Check: `{ngrok_url.replace(':3000', ':8000')}/health`\n",
                "- Chat API: `{ngrok_url.replace(':3000', ':8000')}/chat`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "stop_services"
            },
            "outputs": [],
            "source": [
                "# Stop all services\n",
                "import subprocess\n",
                "import signal\n",
                "import os\n",
                "\n",
                "print(\"üõë Stopping all services...\")\n",
                "\n",
                "# Kill all processes\n",
                "try:\n",
                "    subprocess.run([\"pkill\", \"-f\", \"ollama\"], check=False)\n",
                "    subprocess.run([\"pkill\", \"-f\", \"uvicorn\"], check=False)\n",
                "    subprocess.run([\"pkill\", \"-f\", \"next\"], check=False)\n",
                "    subprocess.run([\"pkill\", \"-f\", \"ngrok\"], check=False)\n",
                "    print(\"‚úÖ All services stopped!\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è  Error stopping services: {e}\")\n",
                "\n",
                "print(\"\\nüìù Note: You can restart the services by running the cells above again.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "troubleshooting"
            },
            "source": [
                "## üîß Troubleshooting\n",
                "\n",
                "### Common Issues:\n",
                "\n",
                "1. **Ollama not starting**:\n",
                "   - Check if Ollama is installed correctly\n",
                "   - Try running `!ollama serve` manually\n",
                "\n",
                "2. **Model download fails**:\n",
                "   - Check internet connection\n",
                "   - Try downloading a smaller model: `!ollama pull gemma:2b`\n",
                "\n",
                "3. **Port conflicts**:\n",
                "   - Change ports in the startup commands\n",
                "   - Check if ports are already in use\n",
                "\n",
                "4. **ngrok not working**:\n",
                "   - Check ngrok dashboard at `http://localhost:4040`\n",
                "   - Restart ngrok if needed\n",
                "\n",
                "5. **Frontend can't connect to backend**:\n",
                "   - Update the API URL in `frontend/hooks/useChat.ts`\n",
                "   - Use the ngrok backend URL instead of localhost\n",
                "\n",
                "### Manual Commands:\n",
                "```bash\n",
                "# Start Ollama\n",
                "ollama serve\n",
                "\n",
                "# Start Backend\n",
                "cd kangtani.ai/backend && uvicorn main:app --host 0.0.0.0 --port 8000\n",
                "\n",
                "# Start Frontend\n",
                "cd kangtani.ai/frontend && pnpm dev --hostname 0.0.0.0 --port 3000\n",
                "\n",
                "# Start ngrok\n",
                "ngrok http 3000\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
