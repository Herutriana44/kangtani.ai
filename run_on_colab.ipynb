{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Run kangtani.ai Backend on Google Colab with ngrok\n",
                "\n",
                "This notebook sets up the FastAPI backend for kangtani.ai, runs Ollama (CPU-only), and exposes the API via ngrok.\n",
                "\n",
                "**Note:** Colab is not ideal for running Ollama LLMs (no GPU, limited RAM/CPU). For demo/testing only.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "#@title 1. Install system dependencies\n",
                "!apt-get update\n",
                "!apt-get install -y ffmpeg git curl unzip\n",
                "!pip install --upgrade pip\n",
                "!pip install fastapi uvicorn[standard] httpx python-multipart openai-whisper PyPDF2 pdfplumber python-docx pyngrok\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "#@title 2. Download and setup Ollama (CPU-only)\n",
                "import os\n",
                "OLLAMA_URL = 'https://github.com/jmorganca/ollama/releases/download/v0.1.32/ollama-linux-amd64'\n",
                "OLLAMA_BIN = '/usr/local/bin/ollama'\n",
                "if not os.path.exists(OLLAMA_BIN):\n",
                "    !curl -L $OLLAMA_URL -o ollama\n",
                "    !chmod +x ollama\n",
                "    !mv ollama $OLLAMA_BIN\n",
                "\n",
                "# Start Ollama in the background\n",
                "import subprocess\n",
                "ollama_proc = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "print('Ollama server started.')\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "#@title 3. Pull the Gemma model (may take a while)\n",
                "!ollama pull gemma\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "#@title 4. Download kangtani.ai backend code\n",
                "# If you have a repo, clone it. Otherwise, download/upload the backend code manually.\n",
                "REPO_URL = 'https://github.com/yourusername/kangtani.ai' # <-- CHANGE THIS if public repo exists\n",
                "if not os.path.exists('kantani.ai'):\n",
                "    !git clone $REPO_URL\n",
                "%cd kantani.ai/backend\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "#@title 5. Start FastAPI backend (port 8000)\n",
                "import threading\n",
                "def run_uvicorn():\n",
                "    import uvicorn\n",
                "    uvicorn.run('main:app', host='0.0.0.0', port=8000, reload=False)\n",
                "\n",
                "threading.Thread(target=run_uvicorn, daemon=True).start()\n",
                "print('FastAPI backend started.')\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "#@title 6. Expose FastAPI backend with ngrok\n",
                "from pyngrok import ngrok\n",
                "public_url = ngrok.connect(8000, 'http')\n",
                "print('ngrok tunnel:', public_url)\n",
                "print('You can now access the API at:', public_url + '/chat')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Usage\n",
                "- Use the public ngrok URL (shown above) as your backend endpoint in the frontend or for API testing.\n",
                "- Example: `POST {ngrok_url}/chat` with `{ \"message\": \"your question\" }`\n",
                "\n",
                "---\n",
                "## Notes\n",
                "- Ollama on Colab is for demo/testing only (slow, CPU-only, limited RAM).\n",
                "- For production, run on your own server with more resources.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}